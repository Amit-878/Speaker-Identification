
# ğŸ™ï¸ Speaker Identification with WhisperX + SpeechBrain

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python\&logoColor=white)](https://www.python.org/)
[![WhisperX](https://img.shields.io/badge/WhisperX-Transcription-orange?logo=openai\&logoColor=white)](https://github.com/m-bain/whisperx)
[![SpeechBrain](https://img.shields.io/badge/SpeechBrain-Speaker%20Verification-green)](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb)
[![HuggingFace](https://img.shields.io/badge/Models-HuggingFace-yellow?logo=huggingface\&logoColor=white)](https://huggingface.co)
[![License](https://img.shields.io/badge/License-MIT-lightgrey)](LICENSE)

![header](https://capsule-render.vercel.app/api?type=waving\&color=gradient\&height=200\&section=header\&text=Meeting+Transcriber\&fontSize=40\&fontAlignY=35\&desc=WhisperX+%2B+SpeechBrain\&descAlignY=55\&animation=fadeIn)

[![Typing SVG](https://readme-typing-svg.herokuapp.com?font=Fira+Code\&duration=3000\&pause=1000\&color=F77D26\&center=true\&vCenter=true\&width=435\&lines=ğŸ™ï¸+Meeting+Transcription;ğŸ‘¥+Speaker+Diarization;ğŸ—£ï¸+Custom+Speaker+Labels)](https://git.io/typing-svg)

Automatically transcribe meetings and assign **real speaker names** using reference audio files.
This project combines:

* **[WhisperX](https://github.com/m-bain/whisperX)** â†’ Fast, accurate speech recognition + forced alignment
* **[SpeechBrain](https://speechbrain.github.io/)** â†’ Speaker verification (ECAPA-TDNN)
* **PyTorch** â†’ Deep learning backend

---

## ğŸš€ Features

âœ… Transcribes meeting audio using **WhisperX**
âœ… Aligns timestamps with word-level precision
âœ… Performs **speaker diarization** (who spoke when)
âœ… Identifies speakers using **reference voice samples**
âœ… Exports transcript in **speaker-labeled text file**

Example Output:

```
[Amit]: Good morning everyone, let's start the meeting.  
[Shivali]: Yes, I wanted to share an update on the project.  
[Unknown]: Sorry, could you repeat that?  
```

---

## ğŸ“‚ Project Structure

```
speaker-identification/
â”‚â”€â”€ main.py                        # Entry point
â”‚â”€â”€ config.py                       # Reads .env for HF token and Whisper settings
â”‚â”€â”€ requirements.txt                # Dependencies
â”‚â”€â”€ README.md                       # Documentation
â”‚â”€â”€ .env                            # Environment variables (HF_TOKEN, Whisper model, etc.)
â”‚â”€â”€ .gitignore                      # Git ignore rules
â”‚
â”œâ”€â”€ refs/                            # Reference speaker audio files
â”‚   â”œâ”€â”€ amit.wav
â”‚   â”œâ”€â”€ shivali.wav
â”‚   â”œâ”€â”€ mudit.wav
â”‚   â””â”€â”€ ...                          # Add more speaker files here
â”‚
â”œâ”€â”€ config/                          # JSON config for references
â”‚   â””â”€â”€ references.json
â”‚
â”œâ”€â”€ src/                             # Source code modules
â”‚   â”œâ”€â”€ pipeline.py                  # High-level orchestration
â”‚   â”œâ”€â”€ speaker_utils.py             # Speaker caching, embedding, and matching
â”‚   â”œâ”€â”€ speaker_verification.py      # Speaker embedding helpers
â”‚   â”œâ”€â”€ audio_utils.py               # Audio conversion / slicing utilities
â”‚   â””â”€â”€ transcription.py             # WhisperX transcription & diarization
â”‚
â””â”€â”€ outputs/                         # Generated transcripts
    â””â”€â”€ <audio_basename>_result.txt  # Example: meeting_result.txt
```

---

## âš¡ Prerequisites

This project depends on models hosted on **Hugging Face**.

1. Python **3.9+**
2. Install [PyTorch](https://pytorch.org/get-started/locally/)
3. Create a Hugging Face account (free) â†’ [Sign up here](https://huggingface.co/join)
4. Generate a **User Access Token** â†’ [settings/tokens](https://huggingface.co/settings/tokens)
5. Accept model licenses on the Hugging Face pages (one-time):

   * WhisperX alignment models: [https://huggingface.co/openai/whisper-large-v3](https://huggingface.co/openai/whisper-large-v3)
   * SpeechBrain ECAPA VoxCeleb: [https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb)

> âš ï¸ If you donâ€™t accept the licenses, the script may pause and request manual approval.

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/speaker-identification.git
cd speaker-identification
```

### 2ï¸âƒ£ Create & activate virtual environment

```bash
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set environment variables

Create `.env` in the root directory:

```env
HF_TOKEN=your_hf_token_here
WHISPER_MODEL=large-v2
WHISPER_LANGUAGE=en
WHISPER_DEVICE=cpu
WHISPER_COMPUTE=int8
SIMILARITY_THRESHOLD=0.37
```

> These settings are now **dynamic**, so you can change the model, device, or threshold without modifying `main.py`.

---

## ğŸ¤ Usage

1. Prepare your meeting audio (`.wav`, `.mp3`, or `.m4a`).
2. Add reference speakers in `config/references.json`:

```json
{
  "Amit": "refs/amit.wav",
  "Shivali": "refs/shivali.wav",
  "Mudit": "refs/mudit.wav"
}
```

3. Run the pipeline:

```bash
python main.py path/to/your_audio_file.wav
```

4. Transcript will be saved to `outputs/<audio_basename>_result.txt`.

> Audio files will be automatically converted to 16kHz WAV if needed.

---

## ğŸ§  Models Used

* **WhisperX** â†’ Speech-to-text + word-level alignment
* **SpeechBrain ECAPA-TDNN** â†’ Speaker verification against reference audios
* **Librosa / SoundFile / PyTorch** â†’ Audio preprocessing and embeddings

---

## ğŸš§ Roadmap

* [x] Support `.wav`, `.mp3`, `.m4a` input with auto-conversion
* [x] Dynamic Whisper model, language, device, compute type via `.env`
* [x] Automatic embeddings cache update when adding/removing/updating reference audios
* [ ] Export transcripts to Word/Excel/JSON
* [ ] Build a simple web UI
* [ ] Support real-time transcription

---

## ğŸ¤ Contributing

Pull requests are welcome!
Report bugs or request features via GitHub issues.
Star the project ğŸŒŸ to support development.

---

## ğŸ“œ License

This project is licensed under **MIT License** â€“ feel free to use and modify.


If you want, I can also **update the â€œExample Workflowâ€ diagrams/snippets** to explicitly show `.mp3/.m4a` conversion in the README for new users. Do you want me to do that?
